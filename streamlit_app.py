# streamlit_app.py
from pathlib import Path
import pickle
import numpy as np
import pandas as pd
import streamlit as st

DEBUG = False 

# ===== í•„ìˆ˜ ê²½ë¡œ =====
DATA_DIR  = Path("data/ml-1m")
ART_DIR   = Path("artifacts")
MODEL_DIR = Path("model")

USERS_FILE   = DATA_DIR / "users.dat"
MOVIES_FILE  = DATA_DIR / "movies.dat"
RATINGS_FILE = DATA_DIR / "ratings.dat"
FIELD_DIMS_PATH = ART_DIR  / "field_dims.npy"
ENCODER_PATH    = ART_DIR  / "label_encoders.pkl"
WEIGHTS_PATH    = MODEL_DIR/ "autoInt_model.weights.h5"   # ë°˜ë“œì‹œ .weights.h5

# ===== Streamlit page config =====
st.set_page_config(page_title="ğŸ¬ MovieLens AutoInt Recommender", layout="wide")

# ===== ë¹ ë¥¸ ìì²´ ì ê²€ =====
missing = [p for p in [FIELD_DIMS_PATH, ENCODER_PATH, WEIGHTS_PATH] if not p.exists()]
if DEBUG:
    st.sidebar.title("ğŸ›  DEBUG")
    st.sidebar.write("CWD:", Path(".").resolve())
    try:
        import sys
        st.sidebar.write("Python:", sys.version)
    except Exception:
        pass
    st.sidebar.write("Root entries:", sorted([p.name for p in Path(".").iterdir()]))

if missing:
    st.error("âŒ ë‹¤ìŒ íŒŒì¼ì´ í•„ìš”í•©ë‹ˆë‹¤(í•™ìŠµ í›„ ìƒì„±ë¨):\n" + "\n".join(str(p) for p in missing))
    st.stop()

# ===== ì•ˆì „í•œ TensorFlow import =====
try:
    import tensorflow as tf
    from tensorflow import keras
    from tensorflow.keras import layers
except Exception as e:
    st.error("TensorFlow ì„í¬íŠ¸ ì‹¤íŒ¨. requirements/runtimeì„ í™•ì¸í•˜ì„¸ìš”.")
    st.exception(e)
    st.stop()

if DEBUG:
    st.sidebar.write("TensorFlow:", tf.__version__)

# ===== ë°ì´í„° ë¡œë”© (ìºì‹œ) =====
@st.cache_data(show_spinner=False)
def load_tables():
    # latin-1 ì¸ì½”ë”©ìœ¼ë¡œ ëª…ì‹œ (UnicodeDecodeError ë°©ì§€)
    users = pd.read_csv(
        USERS_FILE, sep="::", engine="python",
        names=["user_id","gender","age","occupation","zip"],
        encoding="latin-1"
    )
    movies = pd.read_csv(
        MOVIES_FILE, sep="::", engine="python",
        names=["movie_id","title","genres"],
        encoding="latin-1"
    )
    ratings = pd.read_csv(
        RATINGS_FILE, sep="::", engine="python",
        names=["user_id","movie_id","rating","timestamp"],
        encoding="latin-1"
    )
    ratings["label"] = (ratings["rating"] >= 4).astype(int)
    ratings["ts"] = pd.to_datetime(ratings["timestamp"], unit="s")
    ratings["rating_year"]  = ratings["ts"].dt.year
    ratings["rating_month"] = ratings["ts"].dt.month
    movies["main_genre"] = movies["genres"].str.split("|").str[0]
    return users, movies, ratings

# ===== ì•„í‹°íŒ©íŠ¸ & ëª¨ë¸ ë¡œë”© (ìºì‹œ) =====
@st.cache_resource(show_spinner=False)
def load_artifacts_and_model():
    # artifacts ë¡œë“œ
    try:
        field_dims = np.load(FIELD_DIMS_PATH)
        with open(ENCODER_PATH, "rb") as f:
            enc = pickle.load(f)
        # encëŠ” {"cat_cols": [...], "label_encoders": {...}} í˜•íƒœì—¬ì•¼ í•¨
        cat_cols       = enc["cat_cols"]
        label_encoders = enc["label_encoders"]
    except Exception as e:
        st.error("ì•„í‹°íŒ©íŠ¸ ë¡œë“œ ì‹¤íŒ¨(artifacts/*.npy, *.pkl). íŒŒì¼ ë‚´ë¶€ êµ¬ì¡°ë¥¼ í™•ì¸í•˜ì„¸ìš”.")
        raise

    # AutoInt ëª¨ë¸ ê³¨ê²© (í•™ìŠµê³¼ ë™ì¼í•´ì•¼ í•¨)
    num_fields  = len(cat_cols)
    embed_dim   = 32
    num_heads   = 4
    attn_layers = 2
    dropout_rate= 0.2
    mlp_units   = [128, 64]

    inp = keras.Input(shape=(num_fields,), dtype="int32")

    embeds = []
    for i, dim in enumerate(field_dims):
        vi = layers.Lambda(lambda x, idx=i: tf.gather(x, indices=idx, axis=1))(inp)  # (B,)
        vi = layers.Reshape((1,))(vi)
        ei = layers.Embedding(input_dim=int(dim), output_dim=embed_dim)(vi)          # (B,1,E)
        embeds.append(ei)
    E = layers.Concatenate(axis=1)(embeds)  # (B,F,E)

    x = E
    for _ in range(attn_layers):
        attn_out = layers.MultiHeadAttention(num_heads=num_heads, key_dim=embed_dim, dropout=dropout_rate)(x, x)
        x = layers.Add()([x, attn_out])
        x = layers.LayerNormalization()(x)

    x = layers.GlobalAveragePooling1D()(x)
    for u in mlp_units:
        x = layers.Dense(u, activation="relu")(x)
        x = layers.Dropout(dropout_rate)(x)
    out = layers.Dense(1, activation="sigmoid")(x)

    model = keras.Model(inputs=inp, outputs=out)
    model.compile(optimizer="adam", loss="binary_crossentropy")

    # build í›„ ê°€ì¤‘ì¹˜ ë¡œë“œ
    _ = model.predict(np.zeros((1, num_fields), dtype=np.int32), verbose=0)
    try:
        model.load_weights(str(WEIGHTS_PATH))
    except Exception as e:
        st.error("ê°€ì¤‘ì¹˜ ë¡œë“œ ì‹¤íŒ¨(model/autoInt_model.weights.h5). í™•ì¥ì/ê²½ë¡œ/ëª¨ë¸êµ¬ì¡°ë¥¼ í™•ì¸í•˜ì„¸ìš”.")
        raise

    return cat_cols, label_encoders, field_dims, model

# ===== ìœ í‹¸ =====
def map_single(label_encoders, col, val):
    m = label_encoders[col]
    return m.get(str(val), 0)

def recommend_for_user(users, movies, ratings, cat_cols, label_encoders, model, user_id: int, topn: int = 10):
    # ì‚¬ìš©ì íŠ¹ì„±
    u = users[users["user_id"] == user_id]
    if len(u) == 0:
        g, a, o, z = "M", 25, 0, "00000"
    else:
        g, a, o, z = u.iloc[0][["gender","age","occupation","zip"]]

    # ì´ë¯¸ ë³¸ ì˜í™” ì œì™¸
    seen = set(ratings.loc[ratings["user_id"]==user_id, "movie_id"].tolist())
    cand = movies[~movies["movie_id"].isin(seen)].copy()
    if cand.empty:
        return pd.DataFrame(columns=["movie_id","title","genres","score"])

    cand["main_genre"] = cand["genres"].str.split("|").str[0]

    # ì¸ë±ì‹±
    mg_idx = cand["main_genre"].astype(str).map(label_encoders["main_genre"]).fillna(0).astype(int).values
    m_idx  = cand["movie_id"].astype(str).map(label_encoders["movie_id"]).fillna(0).astype(int).values

    g_idx = map_single(label_encoders, "gender", g)
    a_idx = map_single(label_encoders, "age", a)
    o_idx = map_single(label_encoders, "occupation", o)
    z_idx = map_single(label_encoders, "zip", z)
    u_idx = map_single(label_encoders, "user_id", user_id)

    # ì…ë ¥ í–‰ë ¬ (í•™ìŠµ cat_cols ìˆœì„œì™€ ë™ì¼í•´ì•¼ í•¨)
    # ê¸°ë³¸: ["user_id","movie_id","gender","age","occupation","zip","main_genre"]
    n = len(cand)
    U = np.full((n,), u_idx, dtype=np.int32)
    G = np.full((n,), g_idx, dtype=np.int32)
    A = np.full((n,), a_idx, dtype=np.int32)
    O = np.full((n,), o_idx, dtype=np.int32)
    Z = np.full((n,), z_idx, dtype=np.int32)
    X = np.stack([U, m_idx, G, A, O, Z, mg_idx], axis=1)

    scores = model.predict(X, batch_size=65536, verbose=0).ravel()
    out = cand.assign(score=scores).sort_values("score", ascending=False).head(topn)
    return out[["movie_id","title","genres","score"]]

# ===== ë°ì´í„°/ëª¨ë¸ ë¡œë”© =====
users, movies, ratings = load_tables()
cat_cols, label_encoders, field_dims, model = load_artifacts_and_model()

# ===== UI =====
st.title("ğŸ¬ MovieLens AutoInt ì¶”ì²œ")
st.caption("ë°ì´í„°: MovieLens 1M | ëª¨ë¸: AutoInt (TensorFlow/Keras)")

left, mid, right = st.columns([2,2,1])
with left:
    uid = st.selectbox("User ID", options=sorted(users["user_id"].unique().tolist()), index=0)
with mid:
    topn = st.slider("ì¶”ì²œ ê°œìˆ˜", 5, 50, 10, 1)
with right:
    st.write("")

st.divider()
st.markdown("#### ì‚¬ìš©ìì˜ ìµœê·¼ ì‹œì²­ ì´ë ¥(í‰ì  ìˆœ)")
hist = (
    ratings[ratings["user_id"]==uid]
    .sort_values("ts", ascending=False)
    .head(10)
    .merge(movies[["movie_id","title","genres"]], on="movie_id", how="left")
)
st.dataframe(hist[["user_id","movie_id","rating","ts","title","genres"]], use_container_width=True, height=260)

if st.button("ğŸ” ì¶”ì²œ ê²°ê³¼ ë³´ê¸°", type="primary"):
    with st.spinner("ì¶”ì²œ ê³„ì‚° ì¤‘â€¦"):
        recs = recommend_for_user(users, movies, ratings, cat_cols, label_encoders, model, int(uid), topn=topn)
    st.markdown("#### ì¶”ì²œ ê²°ê³¼")
    st.dataframe(recs.reset_index(drop=True), use_container_width=True, height=420)
else:
    st.info("ìƒë‹¨ì—ì„œ ì‚¬ìš©ì/ì¶”ì²œ ê°œìˆ˜ë¥¼ ì„¤ì •í•˜ê³  ë²„íŠ¼ì„ ëˆŒëŸ¬ ì£¼ì„¸ìš”.")

# ===== (ì˜µì…˜) ê°„ë‹¨ ìê°€ ì ê²€ =====
with st.expander("âœ… Self-check (í•„ìš” ì‹œ ì—´ê¸°)"):
    checks = {
        "field_dims.npy": FIELD_DIMS_PATH.exists(),
        "label_encoders.pkl": ENCODER_PATH.exists(),
        "weights (.weights.h5)": WEIGHTS_PATH.exists(),
    }
    st.write({k: ("OK" if v else "MISSING") for k, v in checks.items()})
